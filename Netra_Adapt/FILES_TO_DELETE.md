# Files to Delete - Cleanup Guide

## ğŸ—‘ï¸ SAFE TO DELETE:

### Old Run Results (if you want fresh start):
```bash
rm -rf Run-1/
rm -rf Run-2/
```
These contain results from previous experiments. Delete if you want a clean slate.

---

### Redundant/Unused Scripts:

#### `download_chakshu.py` - KEEP (might be useful)
Has alternative Figshare download method. Keep as backup.

#### `DIAGNOSIS.md` - DELETE
```bash
rm DIAGNOSIS.md
```
Old diagnostic notes, no longer needed.

#### `advanced_analysis.py` - KEEP (optional)
Not used in main pipeline, but useful for post-experiment analysis.

---

### Auto-Generated Files (will be recreated):

#### Results folders (if starting fresh):
```bash
rm -rf results/
rm -rf logs/
```
Will be regenerated when you run the pipeline.

#### CSV files (if you want to regenerate):
```bash
rm -rf data/processed_csvs/
```
Will be regenerated by `prepare_data.py`.

---

## âŒ DO NOT DELETE:

### Core Training Scripts:
- `train_source.py` âœ“
- `train_oracle.py` âœ“
- `adapt_target.py` âœ“
- `evaluate.py` âœ“

### Utilities:
- `models.py` âœ“
- `dataset_loader.py` âœ“
- `utils.py` âœ“
- `training_logger.py` âœ“
- `prepare_data.py` âœ“

### Pipeline Scripts:
- `run_everything.sh` âœ“ (recommended)
- `run_simple.sh` âœ“
- `run_full_pipeline.py` âœ“

### Setup Scripts:
- `setup_complete.sh` âœ“ (NEW - handles credentials)
- `setup_with_download.sh` âœ“ (existing)
- `download_chakshu.py` âœ“ (backup method)

### Documentation:
- `requirements.txt` âœ“
- `QUICK_COMMANDS_INESH.txt` âœ“ (your reference)
- `PIPELINE_USAGE.md` âœ“
- `CHANGES_FOR_RUN2.md` âœ“
- `QUICK_CHANGES_SUMMARY.txt` âœ“

---

## ğŸ§¹ Recommended Cleanup Commands:

```bash
cd /workspace/Netra_Adapt

# Delete old experimental runs (optional)
rm -rf Run-1/ Run-2/

# Delete diagnostic notes
rm -f DIAGNOSIS.md

# Delete old results if starting fresh (optional)
rm -rf results/ logs/

# Delete CSVs to regenerate them (optional)
rm -rf data/processed_csvs/
```

---

## ğŸ“¦ What You Actually Need:

### Minimum files for a clean run:
1. **Training scripts**: `train_*.py`, `adapt_*.py`, `evaluate.py`
2. **Core modules**: `models.py`, `dataset_loader.py`, `utils.py`, `training_logger.py`
3. **Data prep**: `prepare_data.py`
4. **Pipeline**: `run_everything.sh` OR `run_simple.sh`
5. **Setup**: `setup_complete.sh` (if downloading from scratch)
6. **Docs**: `requirements.txt`, `PIPELINE_USAGE.md`

Everything else is either:
- Old results (deletable)
- Documentation (keep for reference)
- Alternative methods (keep as backup)

---

## ğŸ’¾ Storage Space:

| Item | Size | Keep? |
|------|------|-------|
| Raw datasets | ~15 GB | âœ“ Need for training |
| Processed CSVs | ~100 MB | âœ“ Can regenerate |
| Model weights | ~1-2 GB | âœ“ Hours to retrain |
| Results/logs | ~100 MB | Optional |
| Old runs | ~500 MB | Delete |
| Code files | ~5 MB | âœ“ Keep all |

If storage is tight:
1. Keep model weights but delete old run folders
2. Delete intermediate extraction folders after setup
3. Keep CSVs but can delete raw Kaggle zip after extraction

---

## ğŸš€ Starting Fresh (Nuclear Option):

If you want to delete EVERYTHING and start from scratch:

```bash
cd /workspace/Netra_Adapt

# Delete all generated content
rm -rf data/ results/ logs/ Run-*/

# Keep only source code
# Then re-run setup
bash setup_complete.sh
```

This will:
1. Re-download all datasets (~15 GB download)
2. Regenerate all CSVs
3. Start with completely clean slate
4. Takes ~1-2 hours just for setup

**Not recommended unless you have corruption issues!**
