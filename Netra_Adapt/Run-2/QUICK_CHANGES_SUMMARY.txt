═══════════════════════════════════════════════════════════════════════
  QUICK REFERENCE: Changes Made for Second Run
═══════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────┐
│ 1. DATA AUGMENTATION (dataset_loader.py)                           │
└─────────────────────────────────────────────────────────────────────┘
  ✓ Rotation: 20° → 30°
  ✓ NEW: Affine transforms (translate, scale)
  ✓ ColorJitter: brightness/contrast 0.1→0.2
  ✓ NEW: Saturation (0.15) & Hue (0.05) - crucial for pigmentation!
  ✓ NEW: Gaussian blur (p=0.3) - handles image quality variance

┌─────────────────────────────────────────────────────────────────────┐
│ 2. SOURCE TRAINING (train_source.py)                               │
└─────────────────────────────────────────────────────────────────────┘
  Batch size:    32 → 48
  Max epochs:    50 → 60
  Patience:       5 → 8
  Min delta:   1e-4 → 1e-5

┌─────────────────────────────────────────────────────────────────────┐
│ 3. ORACLE TRAINING (train_oracle.py) - MAJOR FIXES                 │
└─────────────────────────────────────────────────────────────────────┘
  Batch size:      24 → 32
  Max epochs:      60 → 80
  Patience:         8 → 12
  Weight decay:  0.05 → 0.01  ← KEY FIX! Was over-regularizing
  Backbone LR:   1e-5 → 2e-5
  Head LR:       1e-3 → 2e-3
  Min delta:     1e-4 → 1e-5

┌─────────────────────────────────────────────────────────────────────┐
│ 4. ADAPTATION (adapt_target.py) - STRATEGY CHANGE                  │
└─────────────────────────────────────────────────────────────────────┘
  Dataset:      Chákṣu TRAIN → Chákṣu TEST  ← Test-Time Adaptation!
  Loss:         Entropy + Diversity → Pure Entropy (removed penalty)
  Batch size:   32 → 48  ← MixEnt needs larger batches
  Max epochs:   25 → 40
  Patience:      5 → 10
  Optimizer:    SGD → AdamW
  Backbone LR:  1e-6 → 5e-6  (5x increase)
  Head LR:      1e-4 → 5e-4  (5x increase)
  NEW: Weight decay 0.001

═══════════════════════════════════════════════════════════════════════

WHY THESE CHANGES:

1. First run failed because:
   ✗ Weak augmentation couldn't handle pigmentation differences
   ✗ Early stopping killed training before convergence
   ✗ Oracle over-regularized (weight_decay too high for small data)
   ✗ Adaptation LR too low, wrong dataset, wrong loss

2. Second run will succeed because:
   ✓ Strong augmentation handles ethnic variation
   ✓ Models train to actual convergence
   ✓ Test-Time Adaptation on actual deployment data
   ✓ Simplified loss lets MixEnt work naturally
   ✓ Proper hyperparameters for each phase

═══════════════════════════════════════════════════════════════════════

EXPECTED RESULTS:

  Run 1 (Failed):                 Run 2 (Expected):
  ─────────────────               ──────────────────
  Pretrained:    0.54             Pretrained:    0.50-0.55
  AIROGS→Chákṣu: 0.50             AIROGS→Chákṣu: 0.60-0.70
  Adapted:       0.50             Adapted:       0.75-0.85 ← IMPROVEMENT!
  Oracle:        0.58             Oracle:        0.80-0.90

═══════════════════════════════════════════════════════════════════════

RUN COMMANDS:

  cd /workspace/Netra_Adapt
  
  # Full pipeline (recommended)
  python train_source.py
  python train_oracle.py
  python adapt_target.py
  python evaluate.py
  
  # Total time: ~4-5 hours

═══════════════════════════════════════════════════════════════════════
